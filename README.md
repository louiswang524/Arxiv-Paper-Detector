# ArXiv Paper Finder and Summarizer (APFS)

A lightweight, privacy-focused tool for discovering and summarizing academic papers from arXiv.org using local Large Language Models (LLMs). Built with cost-effectiveness and offline capability in mind.

## ğŸš€ Features

- **ğŸ” Smart Paper Discovery**: Search arXiv by topic, category, date range, and more
- **ğŸ¤– Local AI Summarization**: Generate summaries using locally hosted LLMs via Ollama
- **ğŸ§  Semantic Search**: Advanced keyword expansion with synonyms and related terms
- **ğŸ“„ Multiple Output Formats**: Console display, tables, Markdown, and JSON export
- **ğŸ”’ Privacy-First**: All processing happens on your device - no cloud APIs
- **ğŸ’° Zero Ongoing Costs**: Free to run after initial setup
- **âš¡ Efficient**: Handles both abstract-only and full-text summarization
- **ğŸ› ï¸ Customizable**: Support for different summary types and LLM models
- **ğŸ¯ Smart Ranking**: Results ranked by semantic relevance to your query

## ğŸ“‹ Requirements

### System Requirements
- **RAM**: Minimum 8 GB (16 GB recommended for larger models)
- **Storage**: ~2-5 GB for LLM models
- **OS**: Windows, macOS, or Linux
- **Python**: 3.10 or higher

### Dependencies
- Ollama (for local LLM inference)
- Python packages (automatically installed)

## âš¡ Quick Start

### 1. Install Ollama

First, install Ollama on your system:

**macOS/Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download and install from https://ollama.ai/

### 2. Pull a Language Model

```bash
ollama pull llama3.2:3b  # Recommended for most users (lower RAM usage)
# OR
ollama pull llama3.1:8b  # Better quality, higher RAM usage
```

### 3. Install APFS

```bash
git clone <repository-url>
cd arxiv-paper-finder
pip install -r requirements.txt
```

### 4. Run Your First Search

```bash
python -m apfs.main "quantum computing" --max-results 5
```

## ğŸ“– Usage

### Basic Search

```bash
# Simple search
python -m apfs.main "large language models"

# Search with category filter
python -m apfs.main "neural networks" --category cs.AI

# Search with date range
python -m apfs.main "transformer architecture" --date-from 2023-01-01 --date-to 2024-12-31
```

### Semantic Search

Enhance your searches with intelligent keyword expansion:

```bash
# Enable semantic search (finds related papers even with different terminology)
python -m apfs.main "AI" --semantic-search

# Different expansion modes
python -m apfs.main "neural networks" --semantic-search --semantic-mode conservative  # Minimal expansion
python -m apfs.main "machine learning" --semantic-search --semantic-mode moderate    # Balanced (default)
python -m apfs.main "NLP" --semantic-search --semantic-mode aggressive              # Maximum expansion

# See how your query gets expanded
python -m apfs.main "deep learning" --semantic-search --explain-search

# Combine with other options
python -m apfs.main "reinforcement learning" --semantic-search --category cs.AI --max-results 15
```

### Advanced Options

```bash
# Use different summary types
python -m apfs.main "machine learning" --summary-type key_findings

# Use full PDF text (slower but more comprehensive)
python -m apfs.main "deep learning" --full-text

# Export results to Markdown
python -m apfs.main "computer vision" --output-format markdown --output-file cv_papers.md

# Use a different model
python -m apfs.main "NLP" --model llama3.1:8b
```

### Complete Example

```bash
python -m apfs.main "attention mechanisms" \
  --max-results 10 \
  --category cs.AI \
  --date-from 2023-01-01 \
  --summary-type general \
  --output-format json \
  --output-file attention_papers.json \
  --full-text
```

## ğŸ› ï¸ Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `query` | Search terms (required) | - |
| `--max-results, -n` | Maximum papers to retrieve | 10 |
| `--category, -c` | ArXiv category filter | None |
| `--date-from` | Start date (YYYY-MM-DD) | None |
| `--date-to` | End date (YYYY-MM-DD) | None |
| `--summarize/--no-summarize` | Generate AI summaries | True |
| `--summary-type` | Summary type | general |
| `--full-text/--abstract-only` | Use full PDF text | False |
| `--model, -m` | Ollama model name | llama3.2:3b |
| `--output-format` | Output format | console |
| `--output-file, -o` | Output file name | Auto-generated |
| `--download-dir` | PDF download directory | Temp directory |
| `--cleanup/--no-cleanup` | Clean up PDFs after processing | True |
| `--semantic-search/--no-semantic-search` | Enable semantic keyword expansion | False |
| `--semantic-mode` | Expansion mode (conservative/moderate/aggressive) | moderate |
| `--explain-search` | Show query expansion explanation | False |

### Semantic Search Modes

- **conservative**: Only direct synonyms and abbreviations (e.g., "AI" â†’ "artificial intelligence")
- **moderate**: Synonyms + common related terms (recommended for most searches)  
- **aggressive**: Maximum expansion with domain-specific terms and concepts

### Summary Types

- **general**: Complete overview with main findings and implications
- **key_findings**: Focus on results and discoveries  
- **methods**: Emphasis on methodology and approach
- **implications**: Analysis of broader impact and significance

### Output Formats

- **console**: Rich formatted display in terminal
- **table**: Compact table view
- **markdown**: Export to Markdown file
- **json**: Machine-readable JSON format

## ğŸ¯ Common ArXiv Categories

| Category | Description |
|----------|-------------|
| `cs.AI` | Artificial Intelligence |
| `cs.LG` | Machine Learning |
| `cs.CL` | Computation and Language |
| `cs.CV` | Computer Vision |
| `physics.gen-ph` | General Physics |
| `math.CO` | Combinatorics |
| `q-bio.QM` | Quantitative Methods |
| `stat.ML` | Machine Learning (Statistics) |

## ğŸ”§ Model Management

### List Available Models
```bash
python -m apfs.main list-models
```

### Pull New Models
```bash
python -m apfs.main pull-model llama3.1:8b
```

### Recommended Models

| Model | RAM Usage | Speed | Quality | Best For |
|-------|-----------|-------|---------|----------|
| `llama3.2:3b` | ~4 GB | Fast | Good | General use, limited resources |
| `llama3.1:8b` | ~8 GB | Medium | Better | Better summaries, more detail |
| `mistral:7b` | ~6 GB | Medium | Good | Alternative to Llama |
| `phi3:mini` | ~2 GB | Very Fast | Basic | Very limited resources |

## ğŸ“ Project Structure

```
arxiv-paper-finder/
â”œâ”€â”€ apfs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # CLI interface
â”‚   â”œâ”€â”€ arxiv_client.py      # ArXiv API integration  
â”‚   â”œâ”€â”€ pdf_handler.py       # PDF download and text extraction
â”‚   â”œâ”€â”€ summarizer.py        # LLM integration
â”‚   â””â”€â”€ output_formatter.py  # Output formatting
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ› Troubleshooting

### Common Issues

**"Model not found" error:**
```bash
ollama pull llama3.2:3b
```

**"No papers found" error:**
- Try broader search terms
- Check category spelling
- Verify date range format (YYYY-MM-DD)

**Memory errors:**
- Use smaller models (phi3:mini, llama3.2:3b)
- Use `--abstract-only` flag
- Reduce `--max-results`

**PDF download failures:**
- Check internet connection
- Some papers may have download restrictions
- Tool automatically falls back to abstract-only

### Performance Tips

1. **For faster performance**: Use `--abstract-only` flag
2. **For better summaries**: Use `--full-text` with larger models
3. **For batch processing**: Increase `--max-results` but monitor RAM usage
4. **For storage**: Use `--cleanup` (default) to remove PDFs after processing

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues and enhancement requests.

## ğŸ“ License

This project is open source. Please check the license file for details.

## ğŸ™ Acknowledgments

- [arXiv](https://arxiv.org/) for providing free access to scientific papers
- [Ollama](https://ollama.ai/) for making local LLM deployment easy
- The open source community for the amazing libraries used in this project

---

**Happy Researching! ğŸ“**